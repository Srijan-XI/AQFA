# Air Quality Analysis and Forecasting

> Time-series analysis and forecasting of Air Quality Index (AQI) using SARIMA, LSTM, and Prophet models.

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)

## ğŸ“‹ Overview

This project explores and forecasts urban air quality using daily city-level data from India (2015â€“2020). The analysis focuses on identifying pollution trends, seasonal patterns, and building predictive models to forecast AQI values 3â€“6 months in advance.

**Key Features:**
- **Comprehensive EDA**: Multi-city analysis with correlation studies and temporal trends
- **Three Forecasting Models**: SARIMA, LSTM (RNN), and Facebook Prophet
- **Complete Testing Framework**: Model comparison with statistical validation
- **Production-Ready**: Modular code with preprocessing utilities and test suite

## ğŸ¯ Use Case

**Problem**: Air pollution causes 7 million premature deaths globally per year. Indian cities like Delhi frequently experience hazardous AQI levels (>400) during winter months.

**Solution**: Accurate AQI forecasting enables:
- Governments to implement timely pollution control measures
- Citizens to plan outdoor activities safely
- Health systems to prepare for pollution-related emergencies

**Target Users**: Environmental scientists, city planners, government bodies, and the public.

## ğŸ“Š Dataset

### Source
- **Kaggle**: [Air Quality Data in India](https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india) (Recommended)
- **Official**: [Central Pollution Control Board (CPCB)](https://cpcb.nic.in/)

### Structure
- **File**: `datasets/city_day.csv`
- **Time Period**: 2015â€“2020
- **Key Columns**: `Date`, `City`, `AQI`, `PM2.5`, `PM10`, `NO2`, `CO`, `SO2`
- **Focus Cities**: Delhi, Jaipur, Guwahati

### Preprocessing
- Date parsing and monthly aggregation
- Forward/backward fill for missing AQI values
- MinMaxScaler normalization for LSTM inputs

## ğŸš€ Quick Start

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd air-quality-analysis-forecasting
```

2. **Create virtual environment** (recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Download dataset**
   - Get `city_day.csv` from [Kaggle](https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india)
   - Place it in `datasets/` directory

### Running the Project

Launch Jupyter Notebook:
```bash
jupyter notebook
```

Then explore the notebooks in sequence:

1. **`notebook/air-quality-analysis.ipynb`** â€” EDA and data exploration
2. **`notebook/sarima-model-AQI-forecasting.ipynb`** â€” SARIMA modeling
3. **`notebook/rnn-lstm-model-AQI-forecasting.ipynb`** â€” LSTM forecasting
4. **`notebook/fb-prophet-air-quality-forecasting.ipynb`** â€” Prophet forecasting
5. **`notebook/test.ipynb`** â€” Comprehensive model testing and comparison

## ğŸ“ Project Structure

```
air-quality-analysis-forecasting/
â”‚
â”œâ”€â”€ datasets/                      # Data files
â”‚   â”œâ”€â”€ city_day.csv              # Main dataset (download required)
â”‚   â”œâ”€â”€ city_hour.csv
â”‚   â”œâ”€â”€ station_day.csv
â”‚   â”œâ”€â”€ station_hour.csv
â”‚   â””â”€â”€ stations.csv
â”‚
â”œâ”€â”€ notebook/                      # Jupyter notebooks
â”‚   â”œâ”€â”€ air-quality-analysis.ipynb              # EDA and exploration
â”‚   â”œâ”€â”€ sarima-model-AQI-forecasting.ipynb      # SARIMA model
â”‚   â”œâ”€â”€ rnn-lstm-model-AQI-forecasting.ipynb    # LSTM model
â”‚   â”œâ”€â”€ fb-prophet-air-quality-forecasting.ipynb # Prophet model
â”‚   â””â”€â”€ test.ipynb                              # Model testing & comparison
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ data_preprocessing.py      # Data preprocessing utilities
â”‚   â””â”€â”€ csv_to_json_chunks.py      # CSV to JSON chunking utility
â”‚
â”œâ”€â”€ web/                           # Web assets (gitignored)
â”‚   â””â”€â”€ json/                      # JSON chunks for web consumption
â”‚       â”œâ”€â”€ city_hour/             # City hourly data chunks
â”‚       â””â”€â”€ station_hour/          # Station hourly data chunks
â”‚
â”œâ”€â”€ tests/                         # Test suite
â”‚   â”œâ”€â”€ test_data_preprocessing.py
â”‚   â””â”€â”€ test-outimg/              # Test output images
â”‚
â”œâ”€â”€ report/                        # Project documentation
â”‚   â””â”€â”€ report.md                 # Detailed project report
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ test_results.csv              # Model comparison results (generated)
â”œâ”€â”€ error_statistics.csv          # Error analysis (generated)
â””â”€â”€ README.md                     # This file
```

## ğŸ§ª Models

### 1. SARIMAX (Statistical)
- **Type**: Classical time-series model
- **Configuration**: Order=(0,1,3), Seasonal=(0,1,1,12)
- **Strengths**: Interpretable, explicit seasonality modeling
- **Use Case**: Baseline model for seasonal data

### 2. LSTM (Deep Learning)
- **Type**: Recurrent Neural Network
- **Configuration**: 2Ã—32 LSTM units, 50 epochs, lookback=12 months
- **Strengths**: Captures non-linear dependencies
- **Use Case**: Complex patterns with sufficient data

### 3. Prophet (Additive Model)
- **Type**: Probabilistic forecasting
- **Configuration**: Yearly seasonality, cmdstanpy backend
- **Strengths**: Robust to outliers, fast iteration, uncertainty intervals
- **Use Case**: Production deployment with automated retraining

## ğŸ“ˆ Model Comparison

| Feature | SARIMAX | LSTM | Prophet |
|---------|---------|------|---------|
| **Seasonality** | âœ… Excellent | âœ… Good | âœ… Excellent |
| **Trend Handling** | Linear/Stochastic | Non-linear | Piecewise Linear |
| **Training Speed** | âš¡ Fast | ğŸŒ Slow | âš¡âš¡ Very Fast |
| **Interpretability** | âœ… High | âŒ Low | âœ… High |
| **Robustness** | âš ï¸ Low | ğŸ”¶ Medium | âœ… High |

*Performance metrics (RMSE, MAE, MAPE) are generated in `test.ipynb` and saved to `test_results.csv`.*

## ğŸ”¬ Testing & Validation

The `test.ipynb` notebook provides comprehensive model evaluation:

- **Residual Diagnostics**: ACF, Q-Q plots, Ljung-Box test
- **Error Analysis**: Distribution, bias, variance
- **Statistical Tests**: Shapiro-Wilk normality test
- **Cross-Validation**: Prophet CV with performance metrics
- **Visual Comparison**: Side-by-side predictions, error plots
- **Final Report**: Automated summary with recommendations

Run the test notebook to generate:
- `test_results.csv` â€” Model performance metrics
- `error_statistics.csv` â€” Detailed error analysis

## ğŸ“Š Key Findings

### Data Insights
- **Seasonality**: Strong winter peaks (Novâ€“Jan), monsoon dips (Julâ€“Sep)
- **Trend**: Increasing pollution levels in Delhi from 2015â€“2020
- **Pollutants**: PM2.5 and PM10 are primary AQI drivers
- **Missing Data**: Handled via forward/backward fill and monthly aggregation

### Model Performance
- **SARIMA**: Best for interpretability and seasonal structure
- **Prophet**: Recommended for production (robustness + uncertainty)
- **LSTM**: Highest potential accuracy with careful tuning
- **Ensemble**: Combination of SARIMA + Prophet recommended for deployment

## ğŸ› ï¸ Dependencies

Core libraries:
- `pandas` â€” Data manipulation
- `numpy` â€” Numerical computing
- `matplotlib`, `seaborn`, `plotly` â€” Visualization
- `statsmodels` â€” SARIMA modeling
- `scikit-learn` â€” Preprocessing & metrics
- `tensorflow` â€” LSTM neural networks
- `prophet` â€” Facebook Prophet forecasting
- `notebook`, `ipywidgets` â€” Jupyter environment

See `requirements.txt` for complete list.

## ğŸ“ Usage Example

```python
import pandas as pd
from src.data_preprocessing import load_and_preprocess

# Load Delhi AQI data
delhi_aqi = load_and_preprocess('datasets/city_day.csv', city='Delhi')

# Train/test split
train = delhi_aqi[:48]  # 48 months
test = delhi_aqi[48:61]  # 13 months

# Fit SARIMA model
from statsmodels.tsa.statespace.sarimax import SARIMAX
model = SARIMAX(train['AQI'], order=(0,1,3), seasonal_order=(0,1,1,12))
results = model.fit()

# Forecast
forecast = results.forecast(steps=len(test))
```

## ğŸ“– Documentation

- **Detailed Report**: See `report/report.md` for comprehensive analysis
- **Notebook Documentation**: Each notebook contains markdown cells with explanations
- **Code Comments**: Source code includes inline documentation

---

## ğŸ”§ Utilities

### CSV to JSON Chunking

For web deployment or data pipeline integration, large CSV files can be split into smaller JSON chunks using the provided utility.

**Script**: `src/csv_to_json_chunks.py`

**Use Case**: Convert massive CSV datasets into manageable JSON files for:
- Loading in web applications (avoid browser memory limits)
- Distributing data across CDN
- Progressive data loading
- API endpoint pagination

**Usage:**

```bash
# Basic usage - split with default settings (50k rows/chunk)
python src/csv_to_json_chunks.py datasets/city_hour.csv

# Custom output directory and chunk size
python src/csv_to_json_chunks.py datasets/city_hour.csv -o web/json/city_hour -r 100000

# With pretty printing (indentation)
python src/csv_to_json_chunks.py datasets/station_hour.csv -o web/json/station_hour -r 100000 --indent 2

# Compressed output
python src/csv_to_json_chunks.py datasets/city_day.csv -o chunks/city_day --compression gz
```

**Available Options:**

| Flag | Description | Default |
|------|-------------|---------|
| `-o`, `--output-dir` | Output directory for JSON chunks | `chunks` |
| `-r`, `--rows-per-chunk` | Number of rows per chunk file | `50000` |
| `--orient` | JSON structure format | `records` |
| `--indent` | Pretty-print indentation (None=compact) | `None` |
| `--compression` | Compress output (`gz`, `bz2`, `zip`, `xz`) | `None` |
| `--ensure-ascii` | Escape non-ASCII characters | `False` |

**Output Format:**

Files are named sequentially: `<basename>.part0.json`, `<basename>.part1.json`, etc.

**Example Output:**
```bash
$ python src/csv_to_json_chunks.py datasets/city_hour.csv -o web/json/city_hour -r 100000
{
  "status": "ok",
  "result": {
    "chunks": 8,
    "rows": 707875,
    "output_dir": "web/json/city_hour",
    "base": "city_hour"
  }
}
```

**Note**: Generated JSON chunks in `web/json/` are gitignored to keep repository size manageable.

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- Additional forecasting models (XGBoost, N-BEATS)
- Multivariate forecasting (include weather data)
- Real-time data integration
- Web dashboard for visualization
- Model deployment pipeline

## ğŸ“„ License

This project is open source and available for educational and research purposes.

## ğŸ™ Acknowledgments

- Dataset: Kaggle & Central Pollution Control Board (CPCB)
- Libraries: statsmodels, TensorFlow, Facebook Prophet communities
- Inspiration: Environmental data science and public health research

## ğŸ“§ Contact

For questions or feedback, please open an issue in the repository.

---

**Built with â¤ï¸ for cleaner air and better public health**
